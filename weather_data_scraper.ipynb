{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from seleniumrequests import Chrome\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "* 1.) Specify year & month combo you want from \"year_month_list\" \n",
    "* 2.) Specify the name of the city by assigning it to \"city_name\"\n",
    "* 3.) Run \"Initiate Scraping\" to start scraping\n",
    "    * Make sure to maximize selenium chrome webpage's screen.\n",
    "    * Code will save each month individually & if code breaks. You can simply rerun. Code will look at what's already scraped and continue where it left off.\n",
    "* 4.) Once done with scraping, run \"Compile Individual Files & Pickle It\" to compile the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_list =[   '2009-09',\n",
    "                     '2009-10',\n",
    "                     '2009-11',\n",
    "                     '2009-12',\n",
    "                     '2010-01',\n",
    "                     '2010-02',\n",
    "                     '2010-03',\n",
    "                     '2010-04',\n",
    "                     '2010-05',\n",
    "                     '2010-06',\n",
    "                     '2010-07',\n",
    "                     '2010-08',\n",
    "                     '2010-09',\n",
    "                     '2010-10',\n",
    "                     '2010-11',\n",
    "                     '2010-12',\n",
    "                     '2011-01',\n",
    "                     '2011-02',\n",
    "                     '2011-03',\n",
    "                     '2011-04',\n",
    "                     '2011-05',\n",
    "                     '2011-06',\n",
    "                     '2011-07',\n",
    "                     '2011-08',\n",
    "                     '2011-09',\n",
    "                     '2011-10',\n",
    "                     '2011-11',\n",
    "                     '2011-12',\n",
    "                     '2012-01',\n",
    "                     '2012-02',\n",
    "                     '2012-03',\n",
    "                     '2012-04',\n",
    "                     '2012-05',\n",
    "                     '2012-06',\n",
    "                     '2012-07',\n",
    "                     '2012-08',\n",
    "                     '2012-09',\n",
    "                     '2012-10',\n",
    "                     '2012-11',\n",
    "                     '2012-12',\n",
    "                     '2013-01',\n",
    "                     '2013-02',\n",
    "                     '2013-03',\n",
    "                     '2013-04',\n",
    "                     '2013-05',\n",
    "                     '2013-06',\n",
    "                     '2013-07',\n",
    "                     '2013-08',\n",
    "                     '2013-09',\n",
    "                     '2013-10',\n",
    "                     '2013-11',\n",
    "                     '2013-12',\n",
    "                     '2014-01',\n",
    "                     '2014-02',\n",
    "                     '2014-03',\n",
    "                     '2014-04',\n",
    "                     '2014-05',\n",
    "                     '2014-06',\n",
    "                     '2014-07',\n",
    "                     '2014-08',\n",
    "                     '2014-09',\n",
    "                     '2014-10',\n",
    "                     '2014-11',\n",
    "                     '2014-12',\n",
    "                     '2015-01',\n",
    "                     '2015-02',\n",
    "                     '2015-03',\n",
    "                     '2015-04',\n",
    "                     '2015-05',\n",
    "                     '2015-06',\n",
    "                     '2015-07',\n",
    "                     '2015-08',\n",
    "                     '2015-09',\n",
    "                     '2015-10',\n",
    "                     '2015-11',\n",
    "                     '2015-12',\n",
    "                     '2016-01',\n",
    "                     '2016-02',\n",
    "                     '2016-03',\n",
    "                     '2016-04',\n",
    "                     '2016-05',\n",
    "                     '2016-06',\n",
    "                     '2016-07',\n",
    "                     '2016-08',\n",
    "                     '2016-09',\n",
    "                     '2016-10',\n",
    "                     '2016-11',\n",
    "                     '2016-12',\n",
    "                     '2017-01',\n",
    "                     '2017-02',\n",
    "                     '2017-03',\n",
    "                     '2017-04',\n",
    "                     '2017-05',\n",
    "                     '2017-06',\n",
    "                     '2017-07',\n",
    "                     '2017-08',\n",
    "                     '2017-09',\n",
    "                     '2017-10',\n",
    "                     '2017-11',\n",
    "                     '2017-12',\n",
    "                     '2018-01',\n",
    "                     '2018-02',\n",
    "                     '2018-03',\n",
    "                     '2018-04',\n",
    "                     '2018-05',\n",
    "                     '2018-06',\n",
    "                     '2018-07',\n",
    "                     '2018-08',\n",
    "                     '2018-09',\n",
    "                     '2018-10',\n",
    "                     '2018-11',\n",
    "                     '2018-12',\n",
    "                     '2019-01',\n",
    "                     '2019-02',\n",
    "                     '2019-03',\n",
    "                     '2019-04',\n",
    "                     '2019-05',\n",
    "                     '2019-06',\n",
    "                     '2019-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = \"chicago\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather_url(url):\n",
    "    # weather data holder to be inserted to pandas dataframe\n",
    "    high_low, weather_desc, humidity_barometer, wind, date_time = [], [], [], [], []\n",
    "    \n",
    "    # open url\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    days_chain = [x.find_all('a') for x in soup.find_all(class_='weatherLinks')]\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Load Entire Page by Scrolling to charts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/3.2);\") # Scroll down\n",
    "    \n",
    "    # First load of each month takes extra long time. Therefore 'counter' variable is used to run else block first\n",
    "    counter = 0\n",
    "    for ix,link in enumerate(days_chain[0]):\n",
    "        \n",
    "        '''\n",
    "        Bottom section tries to solve loading issue by implementing wait feature\n",
    "        Refer : https://selenium-python.readthedocs.io/waits.html\n",
    "        '''\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        if counter!=0:\n",
    "            delay = 3 # seconds\n",
    "            try:\n",
    "                myElem = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'weatherLinks')))\n",
    "            except TimeoutException:\n",
    "                print(\"Loading took too much time!\" ) \n",
    "            day_link = driver.find_element_by_xpath(\"//div[@class='weatherLinks']/a[{}]\".format(ix+1))\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='weatherLinks']/a[{}]\".format(ix+1))))\n",
    "            day_link.click()\n",
    "        else:\n",
    "            delay = 5 # seconds\n",
    "            try:\n",
    "                myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, 'weatherLinks')))\n",
    "            except TimeoutException:\n",
    "                print(\"Loading took too much time!\" ) \n",
    "            day_link = driver.find_element_by_xpath(\"//div[@class='weatherLinks']/a[{}]\".format(ix+1))\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='weatherLinks']/a[{}]\".format(ix+1))))\n",
    "            time.sleep(4)\n",
    "            day_link.click()\n",
    "            time.sleep(3)\n",
    "            counter+=1\n",
    "        \n",
    "        # Wait a bit for the Javascript to fully load data to be scraped\n",
    "        time.sleep(2.5)\n",
    "            \n",
    "        # Scrape weather data\n",
    "        high_low.insert(0,driver.find_elements_by_xpath(\"//div[@class='temp']\")[-1].text) #notice elements, s at the end. This returns a list, and I can index it.\n",
    "        weather_desc.insert(0,driver.find_element_by_xpath(\"//div[@class='wdesc']\").text)\n",
    "        humidity_barometer.insert(0,driver.find_element_by_xpath(\"//div[@class='mid__block']\").text)\n",
    "        wind.insert(0,driver.find_element_by_xpath(\"//div[@class='right__block']\").text)\n",
    "        date_time.insert(0,driver.find_elements_by_xpath(\"//div[@class='date']\")[-1].text)\n",
    "    return high_low, weather_desc, humidity_barometer, wind, date_time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6ebfd756c574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.timeanddate.com/weather/usa/{}/historic?month={}&year={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mhigh_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhumidity_barometer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_weather_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdf_weather_holder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'DATE_TIME'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HIGH_LOW'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhigh_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WEATHER_DESC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mweather_desc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HUMIDITY_BAROMETER'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhumidity_barometer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WIND'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwind\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather_holder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-4a756d30c825>\u001b[0m in \u001b[0;36mscrape_weather_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Wait a bit for the Javascript to fully load data to be scraped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Scrape weather data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initiate Selenium Chrome driver for Mac\n",
    "git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "\n",
    "# Create \"weather_data\" folder if it's not there\n",
    "git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "if 'weather_data' not in os.listdir(git_folder_location):\n",
    "    !mkdir 'weather_data'\n",
    "\n",
    "# already scraped (list from what's already saved in the folder)\n",
    "done_list = os.listdir(git_folder_location+'/weather_data/')\n",
    "if '.DS_Store' in done_list:\n",
    "    done_list.remove('.DS_Store')\n",
    "    \n",
    "#done_list.remove('.DS_Store')\n",
    "done_list_processed = [x[11:][:-4].split('_') for x in done_list]\n",
    "for date in done_list_processed:\n",
    "    if len(date[0])<2:\n",
    "        date[0]='0'+date[0]\n",
    "done_list_processed = [x[1]+'-'+x[0] for x in done_list_processed]\n",
    "\n",
    "# remove done items from list of month & year combo that need to be scraped\n",
    "year_month_list_done_removed = year_month_list.copy()\n",
    "for done_item in done_list_processed:\n",
    "    year_month_list_done_removed.remove(done_item)\n",
    "\n",
    "# iterate through each year & month combo to scrape\n",
    "for date in year_month_list_done_removed:\n",
    "    # define initial empty dataframe\n",
    "    df_weather = pd.DataFrame({'DATE_TIME':[], 'HIGH_LOW':[], 'WEATHER_DESC':[],'HUMIDITY_BAROMETER':[],'WIND':[]})\n",
    "    month = int(date[5:])\n",
    "    year = date[:4]\n",
    "    url = 'https://www.timeanddate.com/weather/usa/{}/historic?month={}&year={}'.format(city_name,month,year)\n",
    "    high_low, weather_desc, humidity_barometer, wind, date_time = scrape_weather_url(url)\n",
    "    df_weather_holder = pd.DataFrame({'DATE_TIME':date_time, 'HIGH_LOW':high_low, 'WEATHER_DESC':weather_desc,'HUMIDITY_BAROMETER':humidity_barometer,'WIND':wind})\n",
    "    df_weather = df_weather.append(df_weather_holder)\n",
    "    df_weather.to_csv('weather_data/df_weather_{}_{}_{}.csv'.format(city_name,month,year),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Individual Files & Pickle It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scraped data files per month\n",
    "done_list = os.listdir(git_folder_location+'/weather_data/')\n",
    "if '.DS_Store' in done_list:\n",
    "    done_list.remove('.DS_Store')\n",
    "\n",
    "# Initiate empty dataframe for weather data to compile individual files\n",
    "df_weather = pd.DataFrame({'DATE_TIME':[], 'HIGH_LOW':[], 'WEATHER_DESC':[],'HUMIDITY_BAROMETER':[],'WIND':[]})\n",
    "\n",
    "# Concat all individual files\n",
    "for file_name in done_list:\n",
    "    file = pd.read_csv('weather_data/'+file_name)\n",
    "    df_weather = pd.concat([df_weather,file],ignore_index=True,axis=0)\n",
    "\n",
    "# Remove duplicate rows. (Mistake created by earlier code imperfection that was corrected later)\n",
    "df_weather.drop_duplicates(inplace=True)\n",
    "\n",
    "# Process and create new columns with individual features for further feature engineering\n",
    "df_weather['DATE_TIME_PROCESSED'] = df_weather['DATE_TIME'].apply(lambda x : x.split(',')[1]+x.split(',')[2])\n",
    "df_weather['DATE_TIME_PROCESSED'] = pd.to_datetime(df_weather['DATE_TIME_PROCESSED'], format = ' %B %d %Y')\n",
    "df_weather['T_HIGH_F'] = df_weather['HIGH_LOW'].apply(lambda x : x.split('/')[0].strip())\n",
    "df_weather['T_LOW_F'] = df_weather['HIGH_LOW'].apply(lambda x : x.split('/')[1].strip().split()[0])\n",
    "df_weather['HUMIDITY_%'] = df_weather['HUMIDITY_BAROMETER'].apply(lambda x : x.split()[1][:-1])\n",
    "df_weather['BAROMETER_HG'] = df_weather['HUMIDITY_BAROMETER'].apply(lambda x : x.split()[3])\n",
    "df_weather['WIND_DIRECTION'] = df_weather['WIND'].apply(lambda x : x.split()[0])\n",
    "df_weather['WIND_MPH'] = df_weather['WIND'].apply(lambda x : x.split()[2])\n",
    "\n",
    "# Drop raw columns that contained multiple featuers as texts\n",
    "df_weather_processed = df_weather.drop(['DATE_TIME', 'HIGH_LOW','HUMIDITY_BAROMETER', 'WIND'], axis=1)\n",
    "\n",
    "# Remove rows for which weather data was not available\n",
    "df_weather_processed = df_weather_processed[df_weather_processed['T_HIGH_F']!='N']\n",
    "\n",
    "# Turn object dtypes to floats for temperatures, humidity, barometer reading, and wind intensity. \n",
    "convert_to_int_list = ['T_LOW_F','T_HIGH_F','HUMIDITY_%','BAROMETER_HG','WIND_MPH']\n",
    "for feature in convert_to_int_list:\n",
    "        df_weather_processed[feature] = df_weather_processed[feature].astype(float)\n",
    "\n",
    "# Save clean data\n",
    "with open('df_weather_clean.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df_weather_processed, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickled File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved file.\n",
    "with open('df_weather_clean.pkl', 'rb') as picklefile: \n",
    "    df_weather_processed = pickle.load(picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
