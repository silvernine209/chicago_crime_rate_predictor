{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from seleniumrequests import Chrome\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Chicago Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing Chicago Crime Data\n",
    "\n",
    "# # Import Chicago Crime Data : Source : https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2\n",
    "# df_crime = pd.read_csv(\"chicago_crime.csv\")\n",
    "\n",
    "# # Convert text date to datetime\n",
    "# df_crime['DATE_TIME'] = pd.to_datetime(df_crime['Date'],format=\"%m/%d/%Y %I:%M:%S %p\", errors = 'coerce')\n",
    "\n",
    "# # Strip hr, min, sec data so that aggregation per day is possible\n",
    "# df_crime['DATE_TIME_TRUN'] = df_crime['DATE_TIME'].map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "# # Drop unneeded columns\n",
    "# df_crime = df_crime[['ID','DATE_TIME_TRUN']].copy()\n",
    "\n",
    "# # Just get the date and total crime count and create new df after aggregating by single day\n",
    "# df_crime = df_crime.groupby(['DATE_TIME_TRUN'],as_index = False).count().copy()\n",
    "\n",
    "# # Save cleaned data\n",
    "# df_crime.to_csv('chicago_crime_cleaned_aggregated.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime = pd.read_csv('chicago_crime_cleaned_aggregated.csv')\n",
    "df_crime['DATE_TIME'] = pd.to_datetime(df_crime['DATE_TIME_TRUN'])\n",
    "df_crime['YEAR_MONTH'] = df_crime['DATE_TIME_TRUN'].apply(lambda x:x[:-3])\n",
    "df_crime = df_crime[df_crime['DATE_TIME_TRUN']>'2009-09-01'] #limited by weather data\n",
    "df_crime.rename(columns={'ID':'CRIME_COUNT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIME_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>824.354399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>152.031129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>716.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>799.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>923.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1538.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIME_COUNT\n",
       "count  3592.000000\n",
       "mean    824.354399\n",
       "std     152.031129\n",
       "min     320.000000\n",
       "25%     716.000000\n",
       "50%     799.000000\n",
       "75%     923.000000\n",
       "max    1538.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME_TRUN</th>\n",
       "      <th>CRIME_COUNT</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>YEAR_MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>2009-09-02</td>\n",
       "      <td>1105</td>\n",
       "      <td>2009-09-02</td>\n",
       "      <td>2009-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>2009-09-03</td>\n",
       "      <td>1125</td>\n",
       "      <td>2009-09-03</td>\n",
       "      <td>2009-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>2009-09-04</td>\n",
       "      <td>1172</td>\n",
       "      <td>2009-09-04</td>\n",
       "      <td>2009-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>2009-09-05</td>\n",
       "      <td>1067</td>\n",
       "      <td>2009-09-05</td>\n",
       "      <td>2009-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>2009-09-06</td>\n",
       "      <td>1105</td>\n",
       "      <td>2009-09-06</td>\n",
       "      <td>2009-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DATE_TIME_TRUN  CRIME_COUNT  DATE_TIME YEAR_MONTH\n",
       "3166     2009-09-02         1105 2009-09-02    2009-09\n",
       "3167     2009-09-03         1125 2009-09-03    2009-09\n",
       "3168     2009-09-04         1172 2009-09-04    2009-09\n",
       "3169     2009-09-05         1067 2009-09-05    2009-09\n",
       "3170     2009-09-06         1105 2009-09-06    2009-09"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Chrome browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Selenium Chrome driver for Mac\n",
    "git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "# url = 'https://www.timeanddate.com/weather/usa/chicago/historic?month={}&year={}'.format(2,2010)\n",
    "# driver.get(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Testing) scroll length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.timeanddate.com/weather/usa/chicago/historic?month={}&year={}'.format(2,2010)\n",
    "# driver.get(url) \n",
    "# # Load Entire Page by Scrolling to charts\n",
    "# SCROLL_PAUSE_TIME = 2# Scroll to Very Bottom to Load All\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/4);\") # Scroll down to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather_url(url):\n",
    "    # weather data holder\n",
    "    high_low, weather_desc, humidity_barometer, wind, date_time = [], [], [], [], []\n",
    "    # open url\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    days_chain = [x.find_all('a') for x in soup.find_all(class_='weatherLinks')]\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Load Entire Page by Scrolling to charts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/3.5);\") # Scroll down to bottom\n",
    "\n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "    for ix,link in enumerate(days_chain[0]):\n",
    "        \n",
    "        '''\n",
    "        Bottom section tries to solve loading issue\n",
    "        Refer : https://selenium-python.readthedocs.io/waits.html\n",
    "        '''\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        if counter!=0:\n",
    "            delay = 3 # seconds\n",
    "            try:\n",
    "                myElem = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'weatherLinks')))\n",
    "            except TimeoutException:\n",
    "                print(\"Loading took too much time!\" ) \n",
    "            day_link = driver.find_element_by_xpath(\"//div[@class='weatherLinks']/a[{}]\".format(ix+1))\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='weatherLinks']/a[{}]\".format(ix+1))))\n",
    "            day_link.click()\n",
    "        else:\n",
    "            delay = 5 # seconds\n",
    "            try:\n",
    "                myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, 'weatherLinks')))\n",
    "            except TimeoutException:\n",
    "                print(\"Loading took too much time!\" ) \n",
    "            day_link = driver.find_element_by_xpath(\"//div[@class='weatherLinks']/a[{}]\".format(ix+1))\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='weatherLinks']/a[{}]\".format(ix+1))))\n",
    "            time.sleep(4)\n",
    "            day_link.click()\n",
    "            time.sleep(3)\n",
    "            counter+=1\n",
    "\n",
    "        time.sleep(2.5)\n",
    "            \n",
    "        # Selenium locating element document : https://selenium-python.readthedocs.io/locating-elements.html#locating-elements\n",
    "#         try:\n",
    "#             myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, 'right__block')))\n",
    "#         except TimeoutException:\n",
    "#             print(\"Loading took too much time!\")\n",
    "        high_low.insert(0,driver.find_elements_by_xpath(\"//div[@class='temp']\")[-1].text) #notice elements, s at the end. This returns a list, and I can index it.\n",
    "        weather_desc.insert(0,driver.find_element_by_xpath(\"//div[@class='wdesc']\").text)\n",
    "        humidity_barometer.insert(0,driver.find_element_by_xpath(\"//div[@class='mid__block']\").text)\n",
    "        wind.insert(0,driver.find_element_by_xpath(\"//div[@class='right__block']\").text)\n",
    "        date_time.insert(0,driver.find_elements_by_xpath(\"//div[@class='date']\")[-1].text)\n",
    "    return high_low, weather_desc, humidity_barometer, wind, date_time\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove month & year already scraped. Then start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-14c1cbe4d9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.timeanddate.com/weather/usa/chicago/historic?month={}&year={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mhigh_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhumidity_barometer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_weather_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdf_weather_holder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'DATE_TIME'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HIGH_LOW'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhigh_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WEATHER_DESC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mweather_desc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HUMIDITY_BAROMETER'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhumidity_barometer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WIND'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwind\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather_holder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-5ab5ebdd9034>\u001b[0m in \u001b[0;36mscrape_weather_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Selenium locating element document : https://selenium-python.readthedocs.io/locating-elements.html#locating-elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all of month & year combo that need to be scraped\n",
    "year_month_list = list(df_crime['YEAR_MONTH'].unique())\n",
    "\n",
    "# already scraped (list from what's already saved in the folder)\n",
    "done_list = os.listdir(git_folder_location+'/weather_data/')[1:]\n",
    "done_list_processed = [x[11:][:-4].split('_') for x in done_list]\n",
    "for date in done_list_processed:\n",
    "    if len(date[0])<2:\n",
    "        date[0]='0'+date[0]\n",
    "done_list_processed = [x[1]+'-'+x[0] for x in done_list_processed]\n",
    "\n",
    "# remove done items from list of month & year combe that need to be scraped\n",
    "year_month_list_done_removed = year_month_list.copy()\n",
    "for done_item in done_list_processed:\n",
    "    year_month_list_done_removed.remove(done_item)\n",
    "\n",
    "# define initial empty dataframe\n",
    "df_weather = pd.DataFrame({'DATE_TIME':[], 'HIGH_LOW':[], 'WEATHER_DESC':[],'HUMIDITY_BAROMETER':[],'WIND':[]})\n",
    "\n",
    "# iterate through each year & month combo to scrape\n",
    "for date in year_month_list_done_removed:\n",
    "    month = int(date[5:])\n",
    "    year = date[:4]\n",
    "    url = 'https://www.timeanddate.com/weather/usa/chicago/historic?month={}&year={}'.format(month,year)\n",
    "    high_low, weather_desc, humidity_barometer, wind, date_time = scrape_weather_url(url)\n",
    "    df_weather_holder = pd.DataFrame({'DATE_TIME':date_time, 'HIGH_LOW':high_low, 'WEATHER_DESC':weather_desc,'HUMIDITY_BAROMETER':humidity_barometer,'WIND':wind})\n",
    "    df_weather = df_weather.append(df_weather_holder)\n",
    "    df_weather.to_csv('weather_data/df_weather_{}_{}.csv'.format(month,year),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Troubleshooting) Compare saved & actual weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_index = 0\n",
    "\n",
    "# # all of month & year combo that need to be scraped\n",
    "# year_month_list = list(df_crime['YEAR_MONTH'].unique())\n",
    "# # already scraped (list from what's already saved in the folder)\n",
    "# done_list = os.listdir(git_folder_location+'/weather_data/')[1:]\n",
    "# done_list_processed = [x[11:][:-4].split('_') for x in done_list]\n",
    "# for date in done_list_processed:\n",
    "#     if len(date[0])<2:\n",
    "#         date[0]='0'+date[0]\n",
    "# done_list_processed = [x[1]+'-'+x[0] for x in done_list_processed]\n",
    "# # remove done items from list of month & year combe that need to be scraped\n",
    "# year_month_list_done_removed = year_month_list.copy()\n",
    "# for done_item in done_list_processed:\n",
    "#     year_month_list_done_removed.remove(done_item)\n",
    "# # Open weather site for that month\n",
    "# read_month = [x[11:][:-4].split('_') for x in done_list][read_index][0]\n",
    "# read_year = [x[11:][:-4].split('_') for x in done_list][read_index][1]\n",
    "# url = 'https://www.timeanddate.com/weather/usa/chicago/historic?month={}&year={}'.format(read_month,read_year)\n",
    "# git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "# full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "# driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "# driver.get(url) \n",
    "# # Load Entire Page by Scrolling to charts\n",
    "# SCROLL_PAUSE_TIME = 2# Scroll to Very Bottom to Load All\n",
    "# driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/4);\") # Scroll down to bottom\n",
    "# # Load csv file in panda.\n",
    "# pd.read_csv(os.getcwd()+'/weather_data/'+done_list[read_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Next \n",
    "soup.find('a').findNextSibling()\n",
    "### Return a list of all matches\n",
    "soup.find_all('a')  \n",
    "[link for link in soup.find_all('a') if 'joelcoen' in str(link)]\n",
    "### Retrieve the url from an anchor tag\n",
    "soup.find('a')['href']\n",
    "### Find all based on id or class\n",
    "soup.find_all(id='top_links')  \n",
    "soup.find_all(class_='mp_box_content')\n",
    "### Beautiful Soup - Chaining Finds\n",
    "chain = [x.find_all('td') for x in soup.find_all(class_='mp_box_content')]  \n",
    "\n",
    "To extract just the value of interest:   \n",
    "soup.find(class_='mp_box_content').find_all('td')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
