{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from seleniumrequests import Chrome\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn Username: ········\n",
      "LinkedIn Password: ········\n"
     ]
    }
   ],
   "source": [
    "# specifies the path to the chromedriver.exe\n",
    "#driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# # For PC\n",
    "# driver = Chrome()\n",
    "\n",
    "# For Mac\n",
    "git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "\n",
    "# driver.get method() will navigate to a page given by the URL address\n",
    "driver.get('https://www.linkedin.com/login?trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Enter ID\n",
    "username = driver.find_element_by_name(\"session_key\")\n",
    "username_input = getpass.getpass(\"LinkedIn Username: \")\n",
    "username.send_keys(username_input)\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Enter Password\n",
    "password = driver.find_element_by_name(\"session_password\")\n",
    "password_input = getpass.getpass(\"LinkedIn Password: \")\n",
    "password.send_keys(password_input)\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Click Submit Button\n",
    "submit = driver.find_element_by_xpath('//*[@type=\"submit\"]')\n",
    "submit.click()\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Navigate to Metis Alumni page\n",
    "driver.get('https://www.linkedin.com/school/metis/people/?facetGeoRegion=us%3A0')\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Load Entire Page by Scrolling to the Bottom\n",
    "SCROLL_PAUSE_TIME = 2# Scroll to Very Bottom to Load All\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")# Get scroll height\n",
    "while True:    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll down to bottom\n",
    "    time.sleep(SCROLL_PAUSE_TIME) # Wait to load page    \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")# Calculate new scroll height and compare with last scroll height\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load source code of fully loaded page\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Obtain LinkedIn url of All Alumni whose profiles are public\n",
    "linkedin_url_list_dirty = []\n",
    "for m in re.finditer('people_profile_card_name_link',page_source):\n",
    "    linkedin_url_list_dirty.append(page_source[m.end()+8:m.end()+60])\n",
    "linkedin_url_list = [url.split()[0][:-1] for url in linkedin_url_list_dirty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 151 Metis Alumni, but 6 of their profiles are private.  Hence, only 145's linkedin addresses were parsed.\n"
     ]
    }
   ],
   "source": [
    "# Total parsed LinkedIn addresses of Metis Alumni\n",
    "total_num_alumni = page_source.count('artdeco-entity-lockup--stacked-center')\n",
    "total_num_alumni_public = page_source.count('people_profile_card_name_link')\n",
    "print(\"There are {} Metis Alumni, but {} of their profiles are private.  Hence, only {}'s linkedin addresses were parsed.\".format(total_num_alumni,total_num_alumni-total_num_alumni_public,total_num_alumni_public))\n",
    "\n",
    "# # Example of parsed LinkedIn addresses\n",
    "# print(\"Example of parsed LinkedIn addresses :\")\n",
    "# linkedin_url_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test list\n",
    "#linkedin_url_list_temp = [\"/in/matthew-eungoo-lee/\"]\n",
    "linkedin_url_list_temp = [linkedin_url_list[13]]\n",
    "\n",
    "# Function to Scrape data off of LinkedIn profile\n",
    "def scrape_linkedin_profile(url):\n",
    "    # Keep using same tab for each url\n",
    "    driver.get(url)\n",
    "    \n",
    "#     # Create new tap for each url. This block doesn't work. It's not really needed for project though.\n",
    "#     print(url)\n",
    "#     script_text = \"'''window.open('\"+url+ \"','_blank');'''\"\n",
    "#     driver.execute_script(script_text)\n",
    "\n",
    "    # Sleep for Random Seconds Between 1 and 3\n",
    "    time.sleep(random.randint(10,30)/10)\n",
    "    \n",
    "    \n",
    "# Execute scrape_linkedin_profile function for each profile\n",
    "for url in linkedin_url_list_temp: # Replace 'linkedin_url_list_temp' with 'linkedin_url_list' after testing\n",
    "    scrape_linkedin_profile('https://www.linkedin.com'+url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Opening in new tabs. Below two codes work.\n",
    "# driver.execute_script('''window.open(\"https://www.linkedin.com/in/matthew-eungoo-lee/\",'_blank');''')\n",
    "# driver.execute_script('''window.open(\"https://www.google.com\",\"_blank\");''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip from Joe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short summary of how you would likely develop and structure scraping code:\n",
    "1. Starting with one page, write parsing functions that should generalize to any page with the same format.\n",
    "2. Test your code on several different pages to confirm that it works in general. Try to make your code robust by checking returns for None and using try;except clauses to accommodate missing elements like in the get_movie_value function above.\n",
    "3. Figure out the total collection of webpages that you want to scrape, and collect the urls into a list (e.g. 2018 movies 1-100, 101-200, etc.). Iterating through all the urls, request and parse the page, adding the data to a list of dicts.\n",
    "4. Incrementally convert the list of dicts into a dataframe and save it to disk with pd.to_csv().\n",
    "Note that this workflow leaves out some details you may need like intentional pausing, but we'll get to these soon :) \n",
    "Also, this is a suggestion, not a prescription. You should think of project luther as a first foray into setting up a data acquisition pipeline, and it takes some intelligent and creative design to get it right. Our best advice is to start scraping early and see where things break quickly so that you can fix it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LinkedIn Scrape Data](https://www.linkedin.com/pulse/how-easy-scraping-data-from-linkedin-profiles-david-craven/)  \n",
    "[Metis Alumni LinkedIn](https://www.linkedin.com/school/metis/people/?facetGeoRegion=us%3A0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Beautiful Soup is powerful but it has many limitations. If a page needs interactions (like entering password) or if a page is not static, but dynamically generated, we can't use Soup. We will explore other tools for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such tool for tomorrow: *Selenium*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies the path to the chromedriver.exe\n",
    "#driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# # For PC\n",
    "# driver = Chrome()\n",
    "\n",
    "# For Mac\n",
    "git_folder_location = os.path.abspath(os.path.dirname('metis_proj_2_luther'))\n",
    "full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "\n",
    "# driver.get method() will navigate to a page given by the URL address\n",
    "driver.get('https://www.linkedin.com/login?trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Enter ID\n",
    "username = driver.find_element_by_name(\"session_key\")\n",
    "username_input = getpass.getpass(\"LinkedIn Username: \")\n",
    "username.send_keys(username_input)\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Enter Password\n",
    "password = driver.find_element_by_name(\"session_password\")\n",
    "password_input = getpass.getpass(\"LinkedIn Password: \")\n",
    "password.send_keys(password_input)\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Click Submit Button\n",
    "submit = driver.find_element_by_xpath('//*[@type=\"submit\"]')\n",
    "submit.click()\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Navigate to Metis Alumni page\n",
    "driver.get('https://www.linkedin.com/school/metis/people/?facetGeoRegion=us%3A0')\n",
    "\n",
    "# Sleep for Random Seconds Between 1 and 3\n",
    "time.sleep(random.randint(10,30)/10)\n",
    "\n",
    "# Load Entire Page by Scrolling to the Bottom\n",
    "SCROLL_PAUSE_TIME = 2# Scroll to Very Bottom to Load All\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")# Get scroll height\n",
    "while True:    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll down to bottom\n",
    "    time.sleep(SCROLL_PAUSE_TIME) # Wait to load page    \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")# Calculate new scroll height and compare with last scroll height\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Load source code of fully loaded page\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Obtain LinkedIn url of All Alumni whose profiles are public\n",
    "linkedin_url_list_dirty = []\n",
    "for m in re.finditer('people_profile_card_name_link',page_source):\n",
    "    linkedin_url_list_dirty.append(page_source[m.end()+8:m.end()+60])\n",
    "linkedin_url_list = [url.split()[0][:-1] for url in linkedin_url_list_dirty]\n",
    "\n",
    "# Total parsed LinkedIn addresses of Metis Alumni\n",
    "total_num_alumni = page_source.count('artdeco-entity-lockup--stacked-center')\n",
    "total_num_alumni_public = page_source.count('people_profile_card_name_link')\n",
    "print(\"There are {} Metis Alumni, but {} of their profiles are private.  Hence, only {}'s linkedin addresses were parsed.\".format(total_num_alumni,total_num_alumni-total_num_alumni_public,total_num_alumni_public))\n",
    "\n",
    "# # Example of parsed LinkedIn addresses\n",
    "# print(\"Example of parsed LinkedIn addresses :\")\n",
    "# linkedin_url_list[:5]\n",
    "\n",
    "# Test list\n",
    "#linkedin_url_list_temp = [\"/in/matthew-eungoo-lee/\"]\n",
    "linkedin_url_list_temp = [linkedin_url_list[13]]\n",
    "\n",
    "# Function to Scrape data off of LinkedIn profile\n",
    "def scrape_linkedin_profile(url):\n",
    "    # Keep using same tab for each url\n",
    "    driver.get(url)\n",
    "    \n",
    "#     # Create new tap for each url. This block doesn't work. It's not really needed for project though.\n",
    "#     print(url)\n",
    "#     script_text = \"'''window.open('\"+url+ \"','_blank');'''\"\n",
    "#     driver.execute_script(script_text)\n",
    "\n",
    "    # Sleep for Random Seconds Between 1 and 3\n",
    "    time.sleep(random.randint(10,30)/10)\n",
    "    \n",
    "    \n",
    "# Execute scrape_linkedin_profile function for each profile\n",
    "for url in linkedin_url_list_temp: # Replace 'linkedin_url_list_temp' with 'linkedin_url_list' after testing\n",
    "    scrape_linkedin_profile('https://www.linkedin.com'+url)\n",
    "    \n",
    "\n",
    "# # Opening in new tabs. Below two codes work.\n",
    "# driver.execute_script('''window.open(\"https://www.linkedin.com/in/matthew-eungoo-lee/\",'_blank');''')\n",
    "# driver.execute_script('''window.open(\"https://www.google.com\",\"_blank\");''')\n",
    "\n",
    "**Tip from Joe**\n",
    "\n",
    "Here's a short summary of how you would likely develop and structure scraping code:\n",
    "1. Starting with one page, write parsing functions that should generalize to any page with the same format.\n",
    "2. Test your code on several different pages to confirm that it works in general. Try to make your code robust by checking returns for None and using try;except clauses to accommodate missing elements like in the get_movie_value function above.\n",
    "3. Figure out the total collection of webpages that you want to scrape, and collect the urls into a list (e.g. 2018 movies 1-100, 101-200, etc.). Iterating through all the urls, request and parse the page, adding the data to a list of dicts.\n",
    "4. Incrementally convert the list of dicts into a dataframe and save it to disk with pd.to_csv().\n",
    "Note that this workflow leaves out some details you may need like intentional pausing, but we'll get to these soon :) \n",
    "Also, this is a suggestion, not a prescription. You should think of project luther as a first foray into setting up a data acquisition pipeline, and it takes some intelligent and creative design to get it right. Our best advice is to start scraping early and see where things break quickly so that you can fix it! \n",
    "\n",
    "[LinkedIn Scrape Data](https://www.linkedin.com/pulse/how-easy-scraping-data-from-linkedin-profiles-david-craven/)  \n",
    "[Metis Alumni LinkedIn](https://www.linkedin.com/school/metis/people/?facetGeoRegion=us%3A0)\n",
    "\n",
    "\n",
    "\n",
    "Conclusion: Beautiful Soup is powerful but it has many limitations. If a page needs interactions (like entering password) or if a page is not static, but dynamically generated, we can't use Soup. We will explore other tools for that.\n",
    "\n",
    "One such tool for tomorrow: *Selenium*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
